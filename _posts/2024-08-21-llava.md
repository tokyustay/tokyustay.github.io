---
title: "[쇼츠논문리뷰] LLaVA : Visual Instruction Tuning"
date: 2024-08-21 16:47:08 +09:00
categories: [ai, paper review]
tags:
  [
    ai paper review,
    LLaVA,
  ]
---

**선요약**

`LLaVA` : `Large Language and Vision Assistant`로, 이미지 데이터와 GPT-4를 이용해 생성한 텍스트 데이터로 `Multimodal Instruction Tuning` 하여 `language and vision Multimodal` 모델 구현

`Instruction Tuning` : `구체적 지시(instruction)와 그에 대한 응답(response)으로 이루어진 데이터셋`을 이용해 fine-tuning하는 기법. LLM이 human instruction에 대해 자연스럽게 대답할 수 있게 해준다

`Contribution` : instruction에 대한 적절한 response를 생성하게 해주는 LLM의 `Instruction tuning`기법을 `image-text Multimodal` 모델에 적용

<br/>
<br/>

# **Instruction Tuning**

<br/>

**구체적 지시(instruction)와 그에 대한 응답(response)으로 이루어진 데이터셋**을 이용해 모델을 fine-tuning하는 기법

다음에 무슨 말이 올 지 학습한 LLM은 지식 베이스를 구축하였지만 human instruction에 대한 자연스러운 응답을 할 수 없다

이를 해결하기 위해 Instruction-response데이터로 LLM의 지식 베이스를 fine-tuning하여 자연스러운 대답이 가능하게 만든다. 나아가 **LLM 모델의 지식만으로 여러 task를 수행**하게 할 수도 있다

<br/>

LLaVA에선 **image와 text instruction이 input**으로 주어지고 이에 대한 자연스러운 **response를 output**으로 주어 LLM모델을 fine-tuning한다

<br/>

## **데이터 생성**

**Instruction tuning 데이터 생성**에는 멀티모달 데이터셋과 GPT-4를 사용했다

멀티모달 데이터셋에서 이미지의 caption(text data)만을 GPT-4로 주며 instruction-response 데이터셋을 생성하도록 예시와 함께 프롬프트로 지시한다

이를 통해 caption만으로 그럴싸한 데이터를 생성해낼 수 있다(데이터를 생성할 땐 이미지는 사용하지 않음)

**예시**

- **data**: (caption : a group of people standing outside of a black vehicle)
- **prompt**: design a conversation between you and a person asking about this photo
- **GPT4 generate**: (instruction : what color is the vehicle?, response : the color of the vehicle is black)

<br/>

# **LLaVA**

<br/>

![llava](https://llava-vl.github.io/images/llava_arch.png)

<br/>

1. **Input**: *이미지 Xv + 질문(instruction) Xq*

2. **Vision Encoder**: pre-trained CLIP 비전 인코더를 사용하여 이미지의 feature Zv추출

3. **Projection W**: 비전 인코더에서 생성된 Zv를 LLM의 input으로 들어갈 수 있도록 Projection하여 Hv로 만드는 레이어. 논문에선 simple linear layer를 사용하였으나 원한다면 Flamingo 혹은 BLIP-2의 방식도 채택가능하다

4. **Language model**: pre-trained Vicuna 언어 모델을 사용하여 input에 대한 적적한 output response를 생성하도록 fine-tuning

5. **Output**: *응답 Xa*

- 우선 Projection W만 learnable하고 나머지는 frozen한 상태로 Projection layer를 pre-training 한다
- 그 후 Vision Encoder만을 frozen하여 end to end로 모델을 full-training한다

<br/>

이로써 GPT딸깍으로 만들어진 데이터와 linear layer에 때려박은 이미지를 인풋으로  학습할 수 있는 멀티모달 모델이 완성 되었다

실제로 사용해보면 AI에 AI데이터를 더해서 그런지 더 AI스러운 대답을 하며 이미지를 완전히 이해하는 것같진 않다
